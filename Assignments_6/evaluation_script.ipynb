{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Evaluation for the VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchgadgets as tg\n",
    "import os\n",
    "from pathlib import Path as P\n",
    "import numpy as np\n",
    "import utils\n",
    "from models import ConvVAE\n",
    "from tbparse import SummaryReader\n",
    "from matplotlib.gridspec import GridSpec \n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'vae_test'\n",
    "run_name = 'test_2'\n",
    "checkpoint = 10\n",
    "\n",
    "log_dir = P(os.getcwd(), 'experiments', exp_name, run_name, 'logs')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model/Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint was load from: /home/user/denninge/VisionLabSS23/Assignments_6/experiments/vae_test/test_2/checkpoints/checkpoint_10.pth\n"
     ]
    }
   ],
   "source": [
    "# Load the config files\n",
    "load_augm_config_train = utils.load_config('augm_train_preLoad') \n",
    "load_augm_config_test = utils.load_config('augm_test_preLoad')\n",
    "\n",
    "config = utils.load_config_from_run(exp_name, run_name)\n",
    "config['num_iterations'] = config['dataset']['train_size'] // config['batch_size']\n",
    "\n",
    "tg.tools.set_random_seed(config['random_seed'])\n",
    "##-- Load Dataset --##\n",
    "# Simply load the dataset using TorchGadgets and define our dataset to apply the initial augmentations\n",
    "data = tg.data.load_dataset('food101')\n",
    "train_dataset = data['train_dataset']\n",
    "test_dataset = data['test_dataset']\n",
    "train_dataset = tg.data.ImageDataset(dataset=train_dataset, transforms=load_augm_config_train)\n",
    "test_dataset = tg.data.ImageDataset(dataset=test_dataset, transforms=load_augm_config_test, train_set=False)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, drop_last=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=True, drop_last=True, num_workers=2)\n",
    "\n",
    "##-- Load Model from Checkpoint --##\n",
    "model = ConvVAE(input_size=(3,224,224), encoder_layers=config['encoder_layers'], decoder_layers=config['decoder_layers'], latent_dim=config['latent_dim'])\n",
    "utils.load_model_from_checkpoint(exp_name, run_name, model, checkpoint)\n",
    "\n",
    "##-- Data Augmentor --##\n",
    "data_augmentor = tg.data.ImageDataAugmentor(config=config['pre_processing'])\n",
    "\n",
    "##-- Load TensorBoard Logs --##\n",
    "reader = SummaryReader(log_dir, pivot=True)\n",
    "train_data = reader.scalars\n",
    "\n",
    "train_loss = train_data.loc[:,'iteration_metrics/train_loss'].to_numpy().tolist()\n",
    "train_mse= train_data.loc[:,'iteration_metrics/mse'].to_numpy().tolist()\n",
    "train_kld = train_data.loc[:,'iteration_metrics/kld'].to_numpy().tolist()\n",
    "\n",
    "eval_loss = train_data.loc[:,'epoch_metrics/eval_loss'].to_numpy().tolist()\n",
    "eval_mse= train_data.loc[:,'epoch_metrics/mse'].to_numpy().tolist()\n",
    "eval_kld = train_data.loc[:,'epoch_metrics/kld'].to_numpy().tolist()\n",
    "\n",
    "categories = [\n",
    "    'apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare',\n",
    "    'beet_salad', 'beignets', 'bibimbap', 'bread_pudding', 'breakfast_burrito',\n",
    "    'bruschetta', 'caesar_salad', 'cannoli', 'caprese_salad', 'carrot_cake',\n",
    "    'ceviche', 'cheesecake', 'cheese_plate', 'chicken_curry', 'chicken_quesadilla',\n",
    "    'chicken_wings', 'chocolate_cake', 'chocolate_mousse', 'churros', 'clam_chowder',\n",
    "    'club_sandwich', 'crab_cakes', 'creme_brulee', 'croque_madame', 'cup_cakes',\n",
    "    'deviled_eggs', 'donuts', 'dumplings', 'edamame', 'eggs_benedict',\n",
    "    'escargots', 'falafel', 'filet_mignon', 'fish_and_chips', 'foie_gras',\n",
    "    'french_fries', 'french_onion_soup', 'french_toast', 'fried_calamari',\n",
    "    'fried_rice', 'frozen_yogurt', 'garlic_bread', 'gnocchi', 'greek_salad',\n",
    "    'grilled_cheese_sandwich', 'grilled_salmon', 'guacamole', 'gyoza',\n",
    "    'hamburger', 'hot_and_sour_soup', 'hot_dog', 'huevos_rancheros', 'hummus',\n",
    "    'ice_cream', 'lasagna',\n",
    "    'lobster_bisque', 'lobster_roll_sandwich', 'macaroni_and_cheese',\n",
    "    'macarons', 'miso_soup', 'mussels', 'nachos', 'omelette', 'onion_rings',\n",
    "    'oysters', 'pad_thai', 'paella', 'pancakes', 'panna_cotta', 'peking_duck',\n",
    "    'pho', 'pizza', 'pork_chop', 'poutine', 'prime_rib', 'pulled_pork_sandwich',\n",
    "    'ramen', 'ravioli', 'red_velvet_cake', 'risotto', 'samosa', 'sashimi',\n",
    "    'scallops', 'seaweed_salad', 'shrimp_and_grits', 'spaghetti_bolognese',\n",
    "    'spaghetti_carbonara', 'spring_rolls', 'steak', 'strawberry_shortcake',\n",
    "    'sushi', 'tacos', 'takoyaki', 'tiramisu', 'tuna_tartare', 'waffles'\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvVAE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): GELU(approximate='none')\n",
      "    (7): MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): GELU(approximate='none')\n",
      "    (11): MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
      "    (12): Conv2d(256, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (13): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): GELU(approximate='none')\n",
      "    (15): MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
      "    (16): Conv2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): GELU(approximate='none')\n",
      "    (19): AdaptiveAvgPool2d(output_size=[1, 1])\n",
      "    (20): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=512, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Unsqueeze()\n",
      "    (3): Unsqueeze()\n",
      "    (4): Upsample(scale_factor=14.0, mode='nearest')\n",
      "    (5): Reshape()\n",
      "    (6): ConvTranspose2d(512, 512, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): GELU(approximate='none')\n",
      "    (9): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (10): ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): GELU(approximate='none')\n",
      "    (13): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (14): ConvTranspose2d(256, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): GELU(approximate='none')\n",
      "    (17): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (18): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (19): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): GELU(approximate='none')\n",
      "    (21): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (22): ConvTranspose2d(64, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (23): Sigmoid()\n",
      "  )\n",
      "  (fc_mu): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  (fc_sigma): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           4,864\n",
      "       BatchNorm2d-2         [-1, 64, 224, 224]             128\n",
      "              GELU-3         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-4         [-1, 64, 112, 112]               0\n",
      "            Conv2d-5        [-1, 128, 112, 112]         204,928\n",
      "       BatchNorm2d-6        [-1, 128, 112, 112]             256\n",
      "              GELU-7        [-1, 128, 112, 112]               0\n",
      "         MaxPool2d-8          [-1, 128, 56, 56]               0\n",
      "            Conv2d-9          [-1, 256, 56, 56]         819,456\n",
      "      BatchNorm2d-10          [-1, 256, 56, 56]             512\n",
      "             GELU-11          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-12          [-1, 256, 28, 28]               0\n",
      "           Conv2d-13          [-1, 512, 28, 28]       3,277,312\n",
      "      BatchNorm2d-14          [-1, 512, 28, 28]           1,024\n",
      "             GELU-15          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-16          [-1, 512, 14, 14]               0\n",
      "           Conv2d-17          [-1, 512, 14, 14]       6,554,112\n",
      "      BatchNorm2d-18          [-1, 512, 14, 14]           1,024\n",
      "             GELU-19          [-1, 512, 14, 14]               0\n",
      "AdaptiveAvgPool2d-20            [-1, 512, 1, 1]               0\n",
      "          Flatten-21                  [-1, 512]               0\n",
      "           Linear-22                 [-1, 1000]         513,000\n",
      "           Linear-23                 [-1, 1000]         513,000\n",
      "           Linear-24                  [-1, 512]         512,512\n",
      "             GELU-25                  [-1, 512]               0\n",
      "        Unsqueeze-26               [-1, 512, 1]               0\n",
      "        Unsqueeze-27            [-1, 512, 1, 1]               0\n",
      "         Upsample-28          [-1, 512, 14, 14]               0\n",
      "          Reshape-29          [-1, 512, 14, 14]               0\n",
      "  ConvTranspose2d-30          [-1, 512, 14, 14]       6,554,112\n",
      "      BatchNorm2d-31          [-1, 512, 14, 14]           1,024\n",
      "             GELU-32          [-1, 512, 14, 14]               0\n",
      "         Upsample-33          [-1, 512, 28, 28]               0\n",
      "  ConvTranspose2d-34          [-1, 256, 28, 28]       3,277,056\n",
      "      BatchNorm2d-35          [-1, 256, 28, 28]             512\n",
      "             GELU-36          [-1, 256, 28, 28]               0\n",
      "         Upsample-37          [-1, 256, 56, 56]               0\n",
      "  ConvTranspose2d-38          [-1, 128, 56, 56]         819,328\n",
      "      BatchNorm2d-39          [-1, 128, 56, 56]             256\n",
      "             GELU-40          [-1, 128, 56, 56]               0\n",
      "         Upsample-41        [-1, 128, 112, 112]               0\n",
      "  ConvTranspose2d-42         [-1, 64, 112, 112]         204,864\n",
      "      BatchNorm2d-43         [-1, 64, 112, 112]             128\n",
      "             GELU-44         [-1, 64, 112, 112]               0\n",
      "         Upsample-45         [-1, 64, 224, 224]               0\n",
      "  ConvTranspose2d-46          [-1, 3, 224, 224]           4,803\n",
      "          Sigmoid-47          [-1, 3, 224, 224]               0\n",
      "================================================================\n",
      "Total params: 23,264,211\n",
      "Trainable params: 23,264,211\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 235.85\n",
      "Params size (MB): 88.75\n",
      "Estimated Total Size (MB): 325.17\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "summary(model.to('cuda'), (3,224,224))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (15,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m x_ticks \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m,config[\u001b[39m'\u001b[39m\u001b[39mnum_epochs\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m,\u001b[39m4\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m6\u001b[39m,\u001b[39m12\u001b[39m))\n\u001b[0;32m----> 8\u001b[0m smooth_loss \u001b[39m=\u001b[39m tg\u001b[39m.\u001b[39;49mvisualization\u001b[39m.\u001b[39;49msmooth_curve(train_loss, K\u001b[39m=\u001b[39;49m\u001b[39m31\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m smooth_mse \u001b[39m=\u001b[39m tg\u001b[39m.\u001b[39mvisualization\u001b[39m.\u001b[39msmooth_curve(train_mse, K\u001b[39m=\u001b[39m\u001b[39m31\u001b[39m)\n\u001b[1;32m     10\u001b[0m smooth_kld \u001b[39m=\u001b[39m tg\u001b[39m.\u001b[39mvisualization\u001b[39m.\u001b[39msmooth_curve(train_kld, K\u001b[39m=\u001b[39m\u001b[39m31\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/vision_lab/lib/python3.9/site-packages/torchgadgets-0.0.1-py3.9.egg/torchgadgets/visualization/plotting_utils.py:9\u001b[0m, in \u001b[0;36msmooth_curve\u001b[0;34m(f, K)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Smoothing a function using a low-pass filter (mean) of size K \"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m kernel \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones(K) \u001b[39m/\u001b[39m K\n\u001b[0;32m----> 9\u001b[0m f \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate([f[:\u001b[39mint\u001b[39;49m(K\u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m2\u001b[39;49m)], f, f[\u001b[39mint\u001b[39;49m(\u001b[39m-\u001b[39;49mK\u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m2\u001b[39;49m):]])  \u001b[39m# to account for boundaries\u001b[39;00m\n\u001b[1;32m     10\u001b[0m smooth_f \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconvolve(f, kernel, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msame\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m smooth_f \u001b[39m=\u001b[39m smooth_f[K\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m: \u001b[39m-\u001b[39mK\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m]  \u001b[39m# removing boundary-fixes\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (15,) + inhomogeneous part."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "x_ticks_pos = np.arange(0, config['num_iterations']*(config['num_epochs']+1), config['num_iterations'])\n",
    "x_ticks = np.arange(0,config['num_epochs']+1)\n",
    "\n",
    "fig, ax = plt.subplots(1,4, figsize=(12,6))\n",
    "\n",
    "smooth_loss = tg.visualization.smooth_curve(train_loss, K=31)\n",
    "smooth_mse = tg.visualization.smooth_curve(train_mse, K=31)\n",
    "smooth_kld = tg.visualization.smooth_curve(train_kld, K=31)\n",
    "\n",
    "ax[0].plot(train_loss, c=\"blue\", label=\"Loss\", linewidth=3, alpha=0.5)\n",
    "ax[0].plot(smooth_loss, c=\"red\", label=\"Smoothed Loss\", linewidth=3, alpha=1)\n",
    "ax[0].set_xticks(x_ticks_pos, x_ticks)\n",
    "ax[0].legend(loc=\"best\")\n",
    "ax[0].set_xlabel(\"# Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].set_yscale(\"log\")\n",
    "ax[0].set_title(\"Training Loss\")\n",
    "\n",
    "ax[1].plot(smooth_loss, c=\"red\", label=\"Combined Loss\", linewidth=2, alpha=1)\n",
    "ax[1].plot(smooth_mse, c=\"blue\", label=\"MSE\", linewidth=2, alpha=1)\n",
    "ax[1].plot(smooth_kld, c=\"green\", label=\"KL-Divergence\", linewidth=2, alpha=1)\n",
    "ax[1].set_xticks(x_ticks_pos, x_ticks)\n",
    "ax[1].legend(loc=\"best\")\n",
    "ax[1].set_xlabel(\"# Epoch\")\n",
    "ax[1].set_ylabel(\"Loss\")\n",
    "ax[1].set_yscale(\"log\")\n",
    "ax[1].set_title(\"Independent Losses\")\n",
    "\n",
    "ax[2].plot(eval_loss, c=\"red\", label=\"Combined Loss\", linewidth=2, alpha=1)\n",
    "ax[2].plot(eval_mse, c=\"blue\", label=\"MSE\", linewidth=2, alpha=1)\n",
    "ax[2].plot(eval_kld, c=\"green\", label=\"KL-Divergence\", linewidth=2, alpha=1)\n",
    "ax[2].set_xticks(x_ticks)\n",
    "ax[2].legend(loc=\"best\")\n",
    "ax[2].set_xlabel(\"# Epoch\")\n",
    "ax[2].set_ylabel(\"Loss\")\n",
    "ax[2].set_yscale(\"log\")\n",
    "ax[2].set_title(\"Evaluation Loss\")\n",
    "\n",
    "ax[3].plot(train_loss, c=\"red\", label=\"Train Loss\", linewidth=2, alpha=1)\n",
    "ax[3].plot(eval_loss, c=\"green\", label=\"Evaluation Loss\", linewidth=2, alpha=1)\n",
    "ax[3].set_xticks(x_ticks_pos, x_ticks)\n",
    "ax[3].legend(loc=\"best\")\n",
    "ax[3].set_xlabel(\"# Epoch\")\n",
    "ax[3].set_ylabel(\"Loss\")\n",
    "ax[3].set_yscale(\"log\")\n",
    "ax[3].set_title(\"Training/Evaluation Losses\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation RUn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "eval_imgs, eval_output, eval_latents, eval_labels = [], [], [], []\n",
    "with torch.no_grad():\n",
    "    for imgs, lbls in test_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        output, (z, mu, sigma) = model(imgs)\n",
    "        eval_imgs.append(imgs.cpu())\n",
    "        eval_output.append(output.cpu())\n",
    "        eval_latents.append(z.cpu())\n",
    "        eval_labels.append(lbls)\n",
    "\n",
    "model = model.to('cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, output, label = eval_imgs[0], eval_labels[0]\n",
    "\n",
    "\n",
    "# Visualize images and their reconstruction\n",
    "layout = (4,5)\n",
    "num_imgs = layout[0]*layout[1]\n",
    "fig = plt.figure(figsize=(30,15))\n",
    "\n",
    "figure_grid = GridSpec(layout[0], layout[1], figure=fig)\n",
    "\n",
    "img_ind = 0\n",
    "for x in range(layout[0]):\n",
    "    for y in range(layout[1]):\n",
    "        img_plot = figure_grid[x,y].subgridspec(1,2, wspace=0.1, hspace=0.0)\n",
    "        ax = fig.add_subplot(img_plot[0])\n",
    "        ax.imshow(img[img_ind])\n",
    "        ax.axis('off')\n",
    "        ax.set_title('Original')\n",
    "        ax = fig.add_subplot(img_plot[1])\n",
    "        ax.imshow(output[img_ind])\n",
    "        ax.axis('off')\n",
    "        ax.set_title('Reconstructed')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the Latent Space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_projections(points, labels, ax=None, legend=None):\n",
    "    \"\"\" Displaying low-dimensional data projections \"\"\"\n",
    "    \n",
    "    legend = [f\"Class {l}\" for l in np.unique(labels)] if legend is None else legend\n",
    "    if(ax is None):\n",
    "        _, ax = plt.subplots(1,1,figsize=(12,6))\n",
    "    \n",
    "    for i,l in enumerate(np.unique(labels)):\n",
    "        idx = np.where(l==labels)\n",
    "\n",
    "        ax.scatter(points[idx, 0], points[idx, 1], label=legend[int(l)])\n",
    "    ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_display = 2000\n",
    "\n",
    "imgs_flat = np.concatenate([img.view(imgs.shape[0],-1) for img in eval_imgs])\n",
    "latents_np = np.concatenate(eval_latents)\n",
    "labels_np = np.concatenate(eval_labels)\n",
    "\n",
    "pca_imgs = PCA(n_components=2).fit_transform(imgs_flat)\n",
    "\n",
    "# N = 2000\n",
    "fig,ax = plt.subplots(1,2,figsize=(26,8))\n",
    "display_projections(pca_imgs[:num_display], labels_np[:num_display], ax=ax[0], legend=test_dataset.classes)\n",
    "ax[0].set_title(\"PCA Proj. of Images\")\n",
    "display_projections(latents_np[:num_display], labels_np[:num_display], ax=ax[1], legend=test_dataset.classes)\n",
    "ax[1].set_title(\"Encoded Representations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_imgs = TSNE(n_components=2).fit_transform(imgs_flat[:N])\n",
    "tsne_latents = TSNE(n_components=2).fit_transform(latents_np[:N])\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(26,8))\n",
    "display_projections(tsne_imgs[:num_display], labels_np[:num_display], ax=ax[0], legend=test_dataset.classes)\n",
    "ax[0].set_title(\"T-SNE Proj. of Images\")\n",
    "display_projections(tsne_latents[:num_display], labels_np[:num_display], ax=ax[1], legend=test_dataset.classes)\n",
    "ax[1].set_title(\"T-SNE Proj. of Encoded Representations\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation over Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def plot_reconstructed(model, xrange=(-3, 3), yrange=(-2, 2), N=12):\n",
    "    \"\"\"\n",
    "    Sampling equispaced points from the latent space givent the xange and yrange, \n",
    "    decoding latents and visualizing distribution of the space\n",
    "    \"\"\"\n",
    "    SIZE = 32\n",
    "    grid = np.empty((N*SIZE, N*SIZE))\n",
    "    \n",
    "    for i, y in enumerate(np.linspace(*yrange, N)):\n",
    "        for j, x in enumerate(np.linspace(*xrange, N)):\n",
    "            z = torch.Tensor([[x, y]]).to(device)\n",
    "            x_hat = model.decoder(z).cpu()\n",
    "            x_hat = x_hat.view(32,32)\n",
    "            \n",
    "            grid[(N-1-i)*SIZE:(N-i)*SIZE, j*SIZE:(j+1)*SIZE] = x_hat\n",
    "           \n",
    "    plt.figure(figsize=(12,20))\n",
    "    plt.imshow(grid, extent=[*yrange, *xrange], cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reconstructed(model, xrange=(-2, 2), yrange=(-2, 2), N=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
