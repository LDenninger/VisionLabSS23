{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current State: <br/>\n",
    "\n",
    "I copied over all modules needed for training and completing the tasks. I tried putting everything in one notebook, such that it is easier to read and understand for you. I really dislike my code style here but it was rather rushed. Unfortunately, I dont have a stable connection here to debug this notebook on a GPU. The main things that need to be tested are:\n",
    "1. Dataset module: I adapted the dataset from the original GitHub ressource. They also have a complete implementation of a model and everything: https://github.com/CoinCheung/triplet-reid-pytorch/tree/master. If something is not working, it should only require small fixes. You would have to tune and change the data augmentations applied to the images as I do not know if they are useful in our case. The original implementation of the dataset was used for bounding box detection.\n",
    "2. The data mining strategy: I tried to adapt the data mining strategy with major simplifications that we have to make due to the dataset and for easier data loading.\n",
    "The original implementation can be found here: https://github.com/davidsandberg/facenet/blob/096ed770f163957c1e56efa7feeb194773920f6e/src/train_tripletloss.py#L271\n",
    "I rather fly over it and did not found it that helpful as it is written in rather complex TensorFlow code and I did not want to take time to fully understand it.\n",
    "3. The training runs: I think the training runs should work as it is mainly copied from the session.\n",
    "\n",
    "The things that still need to be added are the evaluations and visualizations, but I think that does not need to be too thorough.\n",
    "\n",
    "To work with the dataset, please view the dataset module. There I explained how to download and pre-process the dataset to be working for our use-case.\n",
    "\n",
    "I hope, that it is not too much work to get this running and I did a somewhat good job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def smooth(f, K=5):\n",
    "    \"\"\" Smoothing a function using a low-pass filter (mean) of size K \"\"\"\n",
    "    kernel = np.ones(K) / K\n",
    "    f = np.concatenate([f[:int(K//2)], f, f[int(-K//2):]])  # to account for boundaries\n",
    "    smooth_f = np.convolve(f, kernel, mode=\"same\")\n",
    "    smooth_f = smooth_f[K//2: -K//2]  # removing boundary-fixes\n",
    "    return smooth_f\n",
    "\n",
    "\n",
    "def save_model(model, optimizer, epoch, stats, margin):\n",
    "    \"\"\" Saving model checkpoint \"\"\"\n",
    "    \n",
    "    if(not os.path.exists(\"checkpoints\")):\n",
    "        os.makedirs(\"checkpoints\")\n",
    "    savepath = f\"checkpoints/checkpoint_epoch_{epoch}_margin_{margin}.pth\"\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'stats': stats\n",
    "    }, savepath)\n",
    "    return\n",
    "\n",
    "\n",
    "def load_model(model, optimizer, savepath):\n",
    "    \"\"\" Loading pretrained checkpoint \"\"\"\n",
    "    \n",
    "    checkpoint = torch.load(savepath, map_location=\"cpu\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    stats = checkpoint[\"stats\"]\n",
    "    \n",
    "    return model, optimizer, epoch, stats\n",
    "\n",
    "\n",
    "def count_model_params(model):\n",
    "    \"\"\" Counting the number of learnable parameters in a nn.Module \"\"\"\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return num_params\n",
    "\n",
    "def visualize_progress(train_loss, val_loss, start=0):\n",
    "    \"\"\" Visualizing loss and accuracy \"\"\"\n",
    "    fig, ax = plt.subplots(1,3)\n",
    "    fig.set_size_inches(24,5)\n",
    "\n",
    "    smooth_train = smooth(train_loss, 31)\n",
    "    ax[0].plot(train_loss, c=\"blue\", label=\"Loss\", linewidth=3, alpha=0.5)\n",
    "    ax[0].plot(smooth_train, c=\"red\", label=\"Smoothed Loss\", linewidth=3, alpha=1)\n",
    "    ax[0].legend(loc=\"best\")\n",
    "    ax[0].set_xlabel(\"Iteration\")\n",
    "    ax[0].set_ylabel(\"CE Loss\")\n",
    "    ax[0].set_yscale(\"linear\")\n",
    "    ax[0].set_title(\"Training Progress (linear)\")\n",
    "    \n",
    "    ax[1].plot(train_loss, c=\"blue\", label=\"Loss\", linewidth=3, alpha=0.5)\n",
    "    ax[1].plot(smooth_train, c=\"red\", label=\"Smoothed Loss\", linewidth=3, alpha=1)\n",
    "    ax[1].legend(loc=\"best\")\n",
    "    ax[1].set_xlabel(\"Iteration\")\n",
    "    ax[1].set_ylabel(\"CE Loss\")\n",
    "    ax[1].set_yscale(\"log\")\n",
    "    ax[1].set_title(\"Training Progress (log)\")\n",
    "\n",
    "    smooth_val = smooth(val_loss, 31)\n",
    "    N_ITERS = len(val_loss)\n",
    "    ax[2].plot(np.arange(start, N_ITERS)+start, val_loss[start:], c=\"blue\", label=\"Loss\", linewidth=3, alpha=0.5)\n",
    "    ax[2].plot(np.arange(start, N_ITERS)+start, smooth_val[start:], c=\"red\", label=\"Smoothed Loss\", linewidth=3, alpha=1)\n",
    "    ax[2].legend(loc=\"best\")\n",
    "    ax[2].set_xlabel(\"Iteration\")\n",
    "    ax[2].set_ylabel(\"CE Loss\")\n",
    "    ax[2].set_yscale(\"log\")\n",
    "    ax[2].set_title(f\"Valid Progress\")\n",
    "\n",
    "    return\n",
    "\n",
    "def display_projections(points, labels, ax=None, legend=None):\n",
    "    \"\"\" Displaying low-dimensional data projections \"\"\"\n",
    "    \n",
    "    COLORS = ['r', 'b', 'g', 'y', 'purple', 'orange', 'k', 'brown', 'grey',\n",
    "              'c', \"gold\", \"fuchsia\", \"lime\", \"darkred\", \"tomato\", \"navy\"]\n",
    "    \n",
    "    legend = [f\"Class {l}\" for l in np.unique(labels)] if legend is None else legend\n",
    "    if(ax is None):\n",
    "        _, ax = plt.subplots(1,1,figsize=(12,6))\n",
    "    \n",
    "    for i,l in enumerate(np.unique(labels)):\n",
    "        idx = np.where(l==labels)\n",
    "\n",
    "        ax.scatter(points[idx, 0], points[idx, 1], label=legend[int(l)], c=COLORS[i])\n",
    "    ax.legend(loc=\"best\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Market1501(Dataset):\n",
    "    '''\n",
    "        A dataset wrapper class for the Market1501 dataset.\n",
    "        This class was adapted from https://github.com/CoinCheung/triplet-reid-pytorch/blob/master/datasets/Market1501.py\n",
    "\n",
    "        The dataset has to be initially downloaded using the script: download_dataset.sh.\n",
    "        Unfortunately it seems that the server is down.\n",
    "        The other option is to download it from a Google drive: https://drive.google.com/file/d/0B8-rUzbwVRk0c054eEozWG9COHM/view?pli=1&resourcekey=0-8nyl7K9_x37HlQm34MmrYQ\n",
    "\n",
    "        Before using the DataLoader one needs to run the script: build_meta_file.sh.\n",
    "        This script reads the data and builds a meta file that contains the image names for each person to quickly load positive samples.\n",
    "\n",
    "        The problem of the dataset is the test set that does not contain information about the persons.\n",
    "        We take the images from the query directory as the test set.\n",
    "    '''\n",
    "    def __init__(self, data_path, sample_triplets = True, is_train = True, *args, **kwargs):\n",
    "        super(Market1501, self).__init__(*args, **kwargs)\n",
    "        self.is_train = is_train\n",
    "        self.sample_triplets = sample_triplets\n",
    "        self.data_path = os.path.join(data_path, 'bounding_box_train' if is_train else 'query')\n",
    "        self.imgs = os.listdir(data_path)\n",
    "        self.imgs = [el for el in self.imgs if os.path.splitext(el)[1] == '.jpg']\n",
    "        self.lb_ids = [int(el.split('_')[0]) for el in self.imgs]\n",
    "        self.lb_cams = [int(el.split('_')[1][1]) for el in self.imgs]\n",
    "        self.imgs = [os.path.join(data_path, el) for el in self.imgs]\n",
    "        with open(os.path.join(data_path, 'train_meta_dir.json' if is_train else 'train_meta_dir.json'), 'r') as f:\n",
    "            self.meta_dir = json.load(f)\n",
    "        # Default data augmentation from the original work.\n",
    "        # This could possibly be further tuned.\n",
    "        if is_train:\n",
    "            self.trans = transforms.Compose([\n",
    "                transforms.Resize((288, 144)),\n",
    "                transforms.RandomCrop((256, 128)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.486, 0.459, 0.408), (0.229, 0.224, 0.225)),\n",
    "            ])\n",
    "        else:\n",
    "            self.trans_tuple = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.486, 0.459, 0.408), (0.229, 0.224, 0.225))\n",
    "                ])\n",
    "            self.Lambda = transforms.Lambda(\n",
    "                lambda crops: [self.trans_tuple(crop) for crop in crops])\n",
    "            self.trans = transforms.Compose([\n",
    "                transforms.Resize((288, 144)),\n",
    "                transforms.TenCrop((256, 128)),\n",
    "                self.Lambda,\n",
    "            ])\n",
    "\n",
    "        # useful for sampler\n",
    "        self.lb_img_dict = dict()\n",
    "        self.lb_ids_uniq = set(self.lb_ids)\n",
    "        lb_array = np.array(self.lb_ids)\n",
    "        for lb in self.lb_ids_uniq:\n",
    "            idx = np.where(lb_array == lb)[0]\n",
    "            self.lb_img_dict.update({lb: idx})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.sample_triplets:\n",
    "            return self._return_tiplets(idx)\n",
    "        else:\n",
    "            return self._return_doubles(idx)\n",
    "    \n",
    "    def _return_doubles(self, idx):\n",
    "        anchor_img = Image.open(self.imgs[idx])\n",
    "        anchor_label = self.imgs[idx][:4]\n",
    "\n",
    "        pos_img_paths = self.meta_dir[anchor_label]\n",
    "        pos_img_path = random.choice(pos_img_paths)\n",
    "\n",
    "        # ugly protection from choosing the same image twice\n",
    "        while pos_img_path == self.imgs[idx]:\n",
    "            pos_img_path = random.choice(pos_img_paths)\n",
    "        pos_img = Image.open(pos_img_path)\n",
    "        pos_img_label = self.imgs[idx][:4]\n",
    "  \n",
    "        anchor_img = self.trans(anchor_img)\n",
    "        pos_img = self.trans(pos_img)\n",
    "        neg_img = self.trans(neg_img)\n",
    "\n",
    "        return (anchor_img, pos_img), (anchor_label, pos_img_label)\n",
    "\n",
    "\n",
    "    def _return_tiplets(self, idx):\n",
    "        # This method opens a complete jpg file and returns a PIL image.\n",
    "        # This could potentially be inefficient and slow down training.\n",
    "        # Pre-processing the images prior to training and saving them in PyTorch tensor files might reduce the data loading overhead.\n",
    "        # The dataloader should work with arbitrary image formats. One just needs to adjust Image.open() to torch.load().\n",
    "        anchor_img = Image.open(self.imgs[idx])\n",
    "        anchor_label = self.imgs[idx][:4]\n",
    "\n",
    "        pos_img_paths = self.meta_dir[anchor_label]\n",
    "        pos_img_path = random.choice(pos_img_paths)\n",
    "\n",
    "        # ugly protection from choosing the same image twice\n",
    "        while pos_img_path == self.imgs[idx]:\n",
    "            pos_img_path = random.choice(pos_img_paths)\n",
    "        pos_img = Image.open(pos_img_path)\n",
    "        pos_img_label = self.imgs[idx][:4]\n",
    "        \n",
    "        neg_img_path = random.choice(self.meta_dir[pos_img_label])\n",
    "        # again ugly protection from choosing a negative image from the same person\n",
    "        while neg_img_path[:4]==anchor_label:\n",
    "            neg_img_path = random.choice(self.meta_dir[pos_img_label])\n",
    "        neg_img = Image.open(neg_img_path)\n",
    "        neg_img_label = neg_img_path[:4]\n",
    "\n",
    "        anchor_img = self.trans(anchor_img)\n",
    "        pos_img = self.trans(pos_img)\n",
    "        neg_img = self.trans(neg_img)\n",
    "\n",
    "        return (anchor_img, pos_img, neg_img), (anchor_label, pos_img_label, neg_img_label)\n",
    "    \n",
    "    \n",
    "train_dataset = Market1501(data_path = os.path.join(os.getcwd(),\"Market-1501-v15.09.15\"), is_train = True)\n",
    "test_dataset = Market1501(data_path = os.path.join(os.getcwd(),\"Market-1501-v15.09.15\"), is_train = False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True) \n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils Modules \n",
    "# Here I simply adjusted the implementation of the model presented in the last session.\n",
    "\n",
    "class NormLayer(nn.Module):\n",
    "    \"\"\" Layer that computer embedding normalization \"\"\"\n",
    "    def __init__(self, l=2):\n",
    "        \"\"\" Layer initializer \"\"\"\n",
    "        assert l in [1, 2]\n",
    "        super().__init__()\n",
    "        self.l = l\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Normalizing embeddings x. The shape of x is (B,D) \"\"\"\n",
    "        x_normalized = x / torch.norm(x, p=self.l, dim=-1, keepdim=True)\n",
    "        return x_normalized\n",
    "\n",
    "# An adapted version of the Siamese model that uses a ResNet-18 backbone that can be either pretrained or not.\n",
    "# We remove the classifier layers from the backbone.\n",
    "\n",
    "class SiameseModel(nn.Module):\n",
    "    \"\"\" \n",
    "    Implementation of a simple siamese model \n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim=32, pretrained=False):\n",
    "        \"\"\" Module initializer \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # convolutional feature extractor\n",
    "        resnet = tv.models.resnet18(pretrained=pretrained)\n",
    "        # Remove classification layer\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        # fully connected embedder\n",
    "        self.fc = nn.Linear(512, emb_dim)\n",
    "        \n",
    "        # auxiliar layers\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.norm = NormLayer()\n",
    "    \n",
    "        return\n",
    "    \n",
    "    def forward_one(self, x):\n",
    "        \"\"\" Forwarding just one sample through the model \"\"\"\n",
    "        x = self.cnn(x)\n",
    "        x_flat = self.flatten(x)\n",
    "        x_emb = self.fc(x_flat)\n",
    "        x_emb_norm = self.norm(x_emb)\n",
    "        return x_emb_norm\n",
    "    \n",
    "    def forward(self, anchor, positive, negative=None):\n",
    "        \"\"\" Forwarding a triplet \"\"\"\n",
    "        anchor_emb = self.forward_one(anchor)\n",
    "        positive_emb = self.forward_one(positive)\n",
    "        if negative is not None:\n",
    "            negative_emb = self.forward_one(negative)\n",
    "        \n",
    "        # We could also do the more efficient version as seen below.\n",
    "        # It really depends on the GPU and batch size...\n",
    "        \n",
    "        # imgs = torch.concat([anchor, positive, negative], dim=0)\n",
    "        # embs = self.forward_one(imgs)\n",
    "        # anchor_emb, positive_emb, negative_emb = torch.chunk(embs, 3, dim=0)\n",
    "        if negative is not None:\n",
    "            return anchor_emb, positive_emb, negative_emb\n",
    "        else:\n",
    "            return anchor_emb, positive_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    \"\"\" Implementation of the triplet loss function \"\"\"\n",
    "    def __init__(self, margin=0.2, reduce=\"mean\"):\n",
    "        \"\"\" Module initializer \"\"\"\n",
    "        assert reduce in [\"mean\", \"sum\"]\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        self.reduce = reduce\n",
    "        return\n",
    "        \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        \"\"\" Computing pairwise distances and loss functions \"\"\"\n",
    "        # L2 distances\n",
    "        d_ap = (anchor - positive).pow(2).sum(dim=-1)\n",
    "        d_an = (anchor - negative).pow(2).sum(dim=-1)\n",
    "        \n",
    "        # triplet loss function\n",
    "        loss = (d_ap - d_an + self.margin)\n",
    "        loss = torch.maximum(loss, torch.zeros_like(loss))\n",
    "        \n",
    "        # averaging or summing      \n",
    "        loss = torch.mean(loss) if(self.reduce == \"mean\") else torch.sum(loss)\n",
    "      \n",
    "        return loss\n",
    "\n",
    "class TripletLossWithMining(nn.Module):\n",
    "    def __init__(self, margin=0.2, reduce=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        self.reduce = reduce\n",
    "        return\n",
    "    \n",
    "    def forward(self, anchor, positive, labels):\n",
    "        \"\"\"\n",
    "            The idea here is that we sample an anchor and positive example from the dataset.\n",
    "            Next we compute the distance between the anchor and positive examples.\n",
    "            Then for each anchor, we compute the distance to the other anchors in the batch\n",
    "            and choose a random negative example (ensuring that it is not of the same class as the anchor),\n",
    "            that has a larger distance than the positive example.\n",
    "\n",
    "            Since we have a large number of persons with only a few images per person,\n",
    "            we cannot meet the requirements of minimum number of persons per batch etc.\n",
    "            So I decided to use this simple approach that also makes data loading much easier.\n",
    "\n",
    "            Implementation Comment:\n",
    "                Still, I am not sure how this works in practice and if it properly works,\n",
    "                so I tried to make the implementation expressive enough to further debug it\n",
    "        \"\"\"\n",
    "        d_ap = (anchor - positive).pow(2).sum(dim=-1)\n",
    "        d_an = torch.zeros_like(d_ap)\n",
    "        for i, emb in enumerate(anchor):\n",
    "            neg_emb_dist = (anchor - emb).pow(2).sum(dim=-1)\n",
    "            unequal_label_index = torch.where(labels!= labels[i])[0] # Get indices of the anchors with different labels\n",
    "            unequal_neg_emb_dist = neg_emb_dist[unequal_label_index]\n",
    "            larger_than_pos_index = torch.where(unequal_neg_emb_dist > d_ap[i])[0] # Get the indices of the anchors with different class and larger distances than the positive example\n",
    "            d_an[i] = random.choice(unequal_neg_emb_dist[larger_than_pos_index])\n",
    "\n",
    "        # triplet loss function\n",
    "        loss = (d_ap - d_an + self.margin)\n",
    "        loss = torch.maximum(loss, torch.zeros_like(loss))\n",
    "        \n",
    "        # averaging or summing      \n",
    "        loss = torch.mean(loss) if(self.reduce == \"mean\") else torch.sum(loss)\n",
    "      \n",
    "        return loss\n",
    "\n",
    "# TODO: Implement the other two loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Class for training and validating a siamese model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, criterion, train_loader, valid_loader=None, n_iters=1e4, mining_strategy=False):\n",
    "        \"\"\" Trainer initializer \"\"\"\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        \n",
    "        self.n_iters = int(n_iters)\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-5)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.train_loss = []\n",
    "        self.valid_loss = []\n",
    "\n",
    "        self.mining_strategy = mining_strategy  \n",
    "        return\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def valid_step(self, val_iters=100):\n",
    "        \"\"\" Some validation iterations \"\"\"\n",
    "        self.model.eval()\n",
    "        cur_losses = []\n",
    "        for i, ((anchors, positives, negatives),_) in enumerate(self.valid_loader):   \n",
    "            # setting inputs to GPU\n",
    "            anchors = anchors.to(self.device)\n",
    "            positives = positives.to(self.device)\n",
    "            negatives = negatives.to(self.device)\n",
    "            \n",
    "            # forward pass and triplet loss\n",
    "            anchor_emb, positive_emb, negative_emb = self.model(anchors, positives, negatives)\n",
    "            loss = self.criterion(anchor_emb, positive_emb, negative_emb)\n",
    "            cur_losses.append(loss.item())\n",
    "            \n",
    "            if(i >= val_iters):\n",
    "                break\n",
    "    \n",
    "        self.valid_loss += cur_losses\n",
    "        self.model.train()\n",
    "        \n",
    "        return cur_losses\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def valid_step_with_mining(self, val_iters=100):\n",
    "        \"\"\" Some validation iterations \"\"\"\n",
    "        self.model.eval()\n",
    "        cur_losses = []\n",
    "        for i, ((anchors, positives),(lbl, pos_lbl)) in enumerate(self.valid_loader):   \n",
    "            # setting inputs to GPU\n",
    "            anchors = anchors.to(self.device)\n",
    "            positives = positives.to(self.device)\n",
    "            \n",
    "            # forward pass and triplet loss\n",
    "            anchor_emb, positive_emb = self.model(anchors, positives)\n",
    "            loss = self.criterion(anchor_emb, positive_emb, lbl)\n",
    "            cur_losses.append(loss.item())\n",
    "            \n",
    "            if(i >= val_iters):\n",
    "                break\n",
    "    \n",
    "        self.valid_loss += cur_losses\n",
    "        self.model.train()\n",
    "        \n",
    "        return cur_losses\n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\" Train/Validation loop \"\"\"\n",
    "        if self.mining_strategy:\n",
    "            self._fit_with_mining()\n",
    "        else:\n",
    "            self._fit_without_mining()\n",
    "    \n",
    " \n",
    "\n",
    "    def _fit_without_mining(self):\n",
    "        self.iter_ = 0\n",
    "        progress_bar = tqdm(total=self.n_iters, initial=0)\n",
    "\n",
    "        for i in range(self.n_iters):\n",
    "            for (anchors, positives, negatives), _ in self.train_loader:     \n",
    "                # setting inputs to GPU\n",
    "                anchors = anchors.to(self.device)\n",
    "                positives = positives.to(self.device)\n",
    "                negatives = negatives.to(self.device)\n",
    "\n",
    "                # forward pass and triplet loss\n",
    "\n",
    "                anchor_emb, positive_emb, negative_emb = self.model(anchors, positives, negatives)\n",
    "                loss = self.criterion(anchor_emb, positive_emb, negative_emb)\n",
    "                self.train_loss.append(loss.item())\n",
    "\n",
    "                # optimization\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # updating progress bar\n",
    "                progress_bar.set_description(f\"Train Iter {self.iter_}: Loss={round(loss.item(),5)})\")\n",
    "\n",
    "                # doing some validation every once in a while\n",
    "                if(self.iter_ % 250 == 0):\n",
    "                    cur_losses = self.valid_step()\n",
    "                    print(f\"Valid loss @ iteration {self.iter_}: Loss={np.mean(cur_losses)}\")\n",
    "\n",
    "                self.iter_ = self.iter_+1 \n",
    "                if(self.iter_ >= self.n_iters):\n",
    "                    break\n",
    "            if(self.iter_ >= self.n_iters):\n",
    "                break\n",
    "        return\n",
    "    \n",
    "\n",
    "    def _fit_with_mining(self):\n",
    "        self.iter_ = 0\n",
    "        progress_bar = tqdm(total=self.n_iters, initial=0)\n",
    "\n",
    "        for i in range(self.n_iters):\n",
    "            for (anchors, positives), (lbl, pos_lbl) in self.train_loader:     \n",
    "                # setting inputs to GPU\n",
    "                anchors = anchors.to(self.device)\n",
    "                positives = positives.to(self.device)\n",
    "\n",
    "                # forward pass and triplet loss\n",
    "\n",
    "                anchor_emb, positive_emb = self.model(anchors, positives)\n",
    "                loss = self.criterion(anchor_emb, positive_emb, lbl)\n",
    "                self.train_loss.append(loss.item())\n",
    "\n",
    "                # optimization\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # updating progress bar\n",
    "                progress_bar.set_description(f\"Train Iter {self.iter_}: Loss={round(loss.item(),5)})\")\n",
    "\n",
    "                # doing some validation every once in a while\n",
    "                if(self.iter_ % 250 == 0):\n",
    "                    cur_losses = self.valid_step()\n",
    "                    print(f\"Valid loss @ iteration {self.iter_}: Loss={np.mean(cur_losses)}\")\n",
    "\n",
    "                self.iter_ = self.iter_+1 \n",
    "                if(self.iter_ >= self.n_iters):\n",
    "                    break\n",
    "            if(self.iter_ >= self.n_iters):\n",
    "                break\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training: Pre-trained ResNet-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseModel()\n",
    "criterion = TripletLoss(margin=0.2)\n",
    "trainer = Trainer(model=model, criterion=criterion, train_loader=train_loader, valid_loader=test_loader, n_iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_progress(trainer.train_loss, trainer.valid_loss, start=120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {\n",
    "    \"train_loss\": trainer.train_loss,\n",
    "    \"valid_loss\": trainer.valid_loss\n",
    "}\n",
    "save_model(trainer.model, trainer.optimizer, trainer.iter_, stats, margin=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training: ResNet-18 from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = SiameseModel()\n",
    "criterion = TripletLoss(margin=0.2)\n",
    "trainer = Trainer(model=model, criterion=criterion, train_loader=train_loader, valid_loader=test_loader, n_iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_progress(trainer.train_loss, trainer.valid_loss, start=120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {\n",
    "    \"train_loss\": trainer.train_loss,\n",
    "    \"valid_loss\": trainer.valid_loss\n",
    "}\n",
    "save_model(trainer.model, trainer.optimizer, trainer.iter_, stats, margin=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare models qualitatively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training: Best model using semi-hard negative mining strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The semi-hard negative mining strategy is implemented within the loss function.\n",
    "# The Triplet loss function with mining was not properly tested, so you might need to change something.\n",
    "# I am not that happy with the implementation, if you want you can also change it completely.\n",
    "train_dataset2 = Market1501(data_path = os.path.join(os.getcwd(),\"Market-1501-v15.09.15\"), is_train = True)\n",
    "test_dataset2 = Market1501(data_path = os.path.join(os.getcwd(),\"Market-1501-v15.09.15\"), is_train = False)\n",
    "\n",
    "train_loader2 = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128, sample_triplets=False, shuffle=True) \n",
    "test_loader2 = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=128, shuffle=False) \n",
    "\n",
    "model = SiameseModel()\n",
    "criterion = TripletLossWithMining(margin=0.2)\n",
    "trainer = Trainer(model=model, criterion=criterion, train_loader=train_loader2, valid_loader=test_loader2, n_iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_progress(trainer.train_loss, trainer.valid_loss, start=120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {\n",
    "    \"train_loss\": trainer.train_loss,\n",
    "    \"valid_loss\": trainer.valid_loss\n",
    "}\n",
    "save_model(trainer.model, trainer.optimizer, trainer.iter_, stats, margin=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate the models and compare them. One might need to rename the previous trainers such that they do not overwrite each other and we can compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the embeddings. I copied the functions from the session already into the notebook."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
